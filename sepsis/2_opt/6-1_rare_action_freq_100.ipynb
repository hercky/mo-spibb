{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cvxpy as cp\n",
    "import time\n",
    "\n",
    "from utils import reward_direct_policy_evaluation, cost_direct_policy_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The defaults go here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA_R = 1.0\n",
    "LAMBDA_C = 1.0\n",
    "\n",
    "COEFFS = (LAMBDA_R, LAMBDA_C)\n",
    "\n",
    "EPS = 2.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Defaults MDP parameters -------------\n",
    "nS, nA = 750, 25\n",
    "DEATH_STATE = 750\n",
    "SURVIVAL_STATE = 751\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "# ----- User args \n",
    "FREQUENCY_THRESHOLD = 100.0\n",
    "COST_FOR_RARE_DECISION = 10.0\n",
    "\n",
    "\n",
    "# -------- Folder Paths -------------\n",
    "basepath = '/enter/path/here'\n",
    "\n",
    "# Path variables\n",
    "IMPORT_PATH = f'{basepath}/m_hat/{SEED}'\n",
    "OUTPUT_PATH = f'{basepath}/output/{SEED}/freq_{FREQUENCY_THRESHOLD}_cost_{COST_FOR_RARE_DECISION}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate the filtered the MDP stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective sample size of train set 14667\n"
     ]
    }
   ],
   "source": [
    "traj_tr = pickle.load(open(f'{IMPORT_PATH}/trajDr_tr.pkl', 'rb'))\n",
    "print('Effective sample size of train set', len(traj_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the defaults for this seed\n",
    "unflitered_P_mat, R_mat = pickle.load(open(f\"{IMPORT_PATH}/MDP_mat.p\", \"rb\"))\n",
    "orig_counts_mat = pickle.load(open(f\"{IMPORT_PATH}/MDP_counts.p\", \"rb\"))\n",
    "\n",
    "# use the orignial cost matrix for unfiltered actions\n",
    "C_mat = pickle.load(open(f\"{OUTPUT_PATH}/C_mat.p\", \"rb\"))\n",
    "pi_baseline = pickle.load(open(f\"{OUTPUT_PATH}/pi_baseline.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "COST_FOR_RARE_DECISION = 10\n",
    "# code to replace all postivie cost value with 0 \n",
    "\n",
    "C_mat[C_mat>0] = COST_FOR_RARE_DECISION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- NOTE: New step of Filtering here ---------\n",
    "\n",
    "# remove the +ve cost from MLE model\n",
    "trans_counts_mat = np.copy(orig_counts_mat)\n",
    "low_freq_idx = trans_counts_mat.sum(axis=-1) < FREQUENCY_THRESHOLD\n",
    "trans_counts_mat[low_freq_idx] = 0\n",
    "\n",
    "\n",
    "# assign absorbing states\n",
    "assert trans_counts_mat[DEATH_STATE, :, :].sum() == 0\n",
    "assert trans_counts_mat[SURVIVAL_STATE, :, :].sum() == 0\n",
    "# Add the death / life absorbing state\n",
    "trans_counts_mat[DEATH_STATE, :, DEATH_STATE] = 1\n",
    "trans_counts_mat[SURVIVAL_STATE, :, SURVIVAL_STATE] = 1\n",
    "\n",
    "# Note: *Not in original paper* send any unobserved actions to death\n",
    "no_tx_idx = trans_counts_mat.sum(axis=-1) == 0\n",
    "trans_counts_mat[no_tx_idx, DEATH_STATE] = 1\n",
    "\n",
    "# Normalise the transition counts\n",
    "# Build probabilistic MDP model\n",
    "\n",
    "# Convert counts into probability\n",
    "P_mat = trans_counts_mat / trans_counts_mat.sum(axis=-1, keepdims=True)\n",
    "assert np.allclose(1, P_mat.sum(axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the solution for this unflitered P_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular PI completed in 00:00:21\n"
     ]
    }
   ],
   "source": [
    "from utils import make_policy_iteration_operator, bounded_successive_approximation, default_termination\n",
    "\n",
    "pi_operator = make_policy_iteration_operator(P=P_mat, R=R_mat, discount=gamma)\n",
    "random_policy = np.ones((nS+2, nA))/nA\n",
    "\n",
    "start_time = time.time()\n",
    "filtered_pi_sol = bounded_successive_approximation(random_policy,\n",
    "                                                   operator=pi_operator,\n",
    "                                                   termination_condition=default_termination,\n",
    "                                                   max_limit= 10,)\n",
    "\n",
    "time_elapsed = time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-start_time))\n",
    "print(f\"Regular PI completed in {time_elapsed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular PI completed in 00:00:51\n"
     ]
    }
   ],
   "source": [
    "from utils import make_reward_shaping_policy_iteration_operator\n",
    "\n",
    "rs_operator = make_reward_shaping_policy_iteration_operator(P=unflitered_P_mat, R=R_mat, C=C_mat, discount=gamma, coeffs=COEFFS)\n",
    "\n",
    "start_time = time.time()\n",
    "pi_rs = bounded_successive_approximation(random_policy, operator=rs_operator,\n",
    "                                                   termination_condition=default_termination,\n",
    "                                                   max_limit= 10,)\n",
    "\n",
    "time_elapsed = time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-start_time))\n",
    "print(f\"Regular PI completed in {time_elapsed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the pre-computed solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sopt_policy(coeffs, eps):    \n",
    "    sol_name = f'cpi_{coeffs[0]}_{coeffs[1]}_{eps}.p'    \n",
    "    pi_solution = pickle.load(open(f'{OUTPUT_PATH}/{sol_name}', 'rb'))    \n",
    "    return pi_solution\n",
    "\n",
    "\n",
    "def load_rs_policy(coeffs):    \n",
    "    sol_name = f'rs_{coeffs[0]}_{coeffs[1]}.p'    \n",
    "    pi_solution = pickle.load(open(f'{OUTPUT_PATH}/{sol_name}', 'rb'))    \n",
    "    return pi_solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_count_sa = orig_counts_mat.sum(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_state_count = orig_count_sa.sum(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical analysis for Unfiltered PI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective sample size of test set 3605\n"
     ]
    }
   ],
   "source": [
    "from eval_utils import Evaluator\n",
    "evaluator = Evaluator(gamma=gamma,\n",
    "                      pi_baseline=pi_baseline,\n",
    "                      C_mat=C_mat,\n",
    "                      cost_for_rare_decision=COST_FOR_RARE_DECISION,\n",
    "                      n_bootstrap=10,)\n",
    "\n",
    "traj_te = pickle.load(open('./trajDr_te.pkl', 'rb'))\n",
    "test_trajectories = evaluator.preprocess_trajecteories(traj_te)\n",
    "\n",
    "N_test = len(test_trajectories)\n",
    "print('Effective sample size of test set', N_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R: 69.53381010466066, C: 53.26411127039579\n"
     ]
    }
   ],
   "source": [
    "# get the mean \n",
    "test_mean_stats = evaluator.get_mean_stats(test_trajectories)\n",
    "print(f'R: {test_mean_stats[0]}, C: {test_mean_stats[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import reward_direct_policy_evaluation, cost_direct_policy_evaluation\n",
    "\n",
    "# ------- Note what P_mat to use?\n",
    "# R_sa = np.einsum('sat,sat -> sa', R_mat, P_mat)\n",
    "R_sa = np.einsum('sat,sat -> sa', R_mat, unflitered_P_mat)\n",
    "\n",
    "\n",
    "def do_ope(pi_solution, mode='DR'):\n",
    "    rV_sol = reward_direct_policy_evaluation(unflitered_P_mat, R_mat, gamma, pi_solution)\n",
    "    rQ_sol = R_sa + gamma * np.einsum('sat,t -> sa', unflitered_P_mat, rV_sol)\n",
    "\n",
    "    cV_sol = cost_direct_policy_evaluation(unflitered_P_mat, C_mat, gamma, pi_solution)\n",
    "    cQ_sol = C_mat + gamma * np.einsum('sat,t -> sa', unflitered_P_mat, cV_sol)\n",
    "\n",
    "\n",
    "    if mode=='DR':\n",
    "        dr_mean_stats, _, _ = evaluator.doubly_robust_ope(test_trajectories, pi_e=pi_solution, \n",
    "                                                                              rQ_e=rQ_sol, cQ_e= cQ_sol)\n",
    "    elif mode == 'WDR':\n",
    "        dr_mean_stats, _, _ = evaluator.weighted_doubly_robust_ope(test_trajectories, pi_e=pi_solution, \n",
    "                                                                              rQ_e=rQ_sol, cQ_e= cQ_sol)\n",
    "\n",
    "    print('-- Mean stats ---')\n",
    "    print(f'R: {dr_mean_stats[0]:0.2f}, C: {dr_mean_stats[1]:0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 346.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Mean stats ---\n",
      "R: 65.95, C: 59.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "do_ope(pi_baseline, mode='DR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 389.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Mean stats ---\n",
      "R: 11.26, C: 18.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# this shows that removing/filtering the rewards can help with reducing cost, but they might also cause Reward performance\n",
    "do_ope(filtered_pi_sol, mode='DR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 98.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Mean stats ---\n",
      "R: 86.75, C: 24.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pi_sopt = load_sopt_policy(COEFFS, EPS)\n",
    "do_ope(pi_sopt, mode='DR')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (mimic)",
   "language": "python",
   "name": "mimic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
